<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects &amp; Publications on Shubham Malaviya</title>
    <link>//localhost:1313/en/engineering/</link>
    <description>Recent content in Projects &amp; Publications on Shubham Malaviya</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 19 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/en/engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Impulse in the Clickstream -Behavioral Insights from Browsing History</title>
      <link>//localhost:1313/en/engineering/impulsive_behavior_modeling/</link>
      <pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/impulsive_behavior_modeling/</guid>
      <description>&lt;h3 id=&#34;-can-we-detect-risky-user-behavior-before-a-phishing-attack-succeeds&#34;&gt;üö© Can we detect risky user behavior before a phishing attack succeeds?&lt;/h3&gt;&#xA;&lt;p&gt;This paper tackles a fundamental but often overlooked problem in modern AI pipelines: annotation errors and their cascading impact on model training, evaluation, and human-in-the-loop data cleaning systems. &lt;br&gt;&#xA;In this research, we introduce a behavioral framework to detect and quantify impulsive clicks at the session level, revealing when users are momentarily vulnerable and how that vulnerability evolves across browsing contexts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toward Scalable Human-in-the-Loop Annotation Error Detection with Noise-Aware Training</title>
      <link>//localhost:1313/en/engineering/annotation_error_detection/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/annotation_error_detection/</guid>
      <description>&lt;h3 id=&#34;-what-if-your-gold-standard-dataset-is-wrongand-your-models-quietly-learn-those-mistakes&#34;&gt;üßπ What if your ‚Äúgold-standard‚Äù dataset is wrong‚Äîand your models quietly learn those mistakes?&lt;/h3&gt;&#xA;&lt;p&gt;This paper tackles a fundamental but often overlooked problem in modern AI pipelines: annotation errors and their cascading impact on model training, evaluation, and human-in-the-loop data cleaning systems. &lt;br&gt;&#xA;We show that annotation error detection (AED) systems themselves break down under label noise, and that simple noise-aware training strategies can dramatically improve their effectiveness without changing model architectures or workflows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unmasking Label Errors: Why Cybersecurity Benchmarks Need Stronger Benchmarks</title>
      <link>//localhost:1313/en/engineering/label_error_ccs/</link>
      <pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/label_error_ccs/</guid>
      <description>&lt;h3 id=&#34;-what-if-your-benchmark-is-wrong-and-your-best-model-only-looks-good-because-of-it&#34;&gt;üß™ What if your benchmark is wrong and your best model only looks good because of it?&lt;/h3&gt;&#xA;&lt;p&gt;This work exposes a critical but underappreciated issue in cybersecurity research: annotation errors in benchmark datasets, particularly in evaluation (test) splits. The study shows that even modest label noise can severely distort model rankings, evaluation metrics, and research conclusions.&lt;br&gt;&#xA;By carefully curating a widely used CTI dataset and re-evaluating modern language models, this work argues for a fundamental shift toward robust, community-validated cybersecurity benchmarks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reducing Communication Overhead in Federated Learning with Parameter-Efficient Finetuning</title>
      <link>//localhost:1313/en/engineering/fl_communication_overhead/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/fl_communication_overhead/</guid>
      <description>&lt;h3 id=&#34;-federated-learning-promises-privacy-but-large-models-make-it-impractical&#34;&gt;üìâ Federated learning promises privacy but large models make it impractical.&lt;/h3&gt;&#xA;&lt;p&gt;This work tackles one of the biggest barriers to deploying pre-trained language models (PLMs) in federated learning (FL): communication overhead. By systematically evaluating parameter-efficient finetuning (PEFT) methods, the work shows that it is possible to reduce communication costs by orders of magnitude without sacrificing model performance.&lt;br&gt;&#xA;The result is a &lt;strong&gt;practical pathway&lt;/strong&gt; for building privacy-preserving NLP systems on real-world, distributed data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>FedFAME: Rethinking Federated Semi-Supervised Learning Without Data Augmentation</title>
      <link>//localhost:1313/en/engineering/fedfame_federated/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/fedfame_federated/</guid>
      <description>&lt;h3 id=&#34;-what-if-federated-learning-didnt-need-labeled-client-data-or-fragile-data-augmentation-tricks&#34;&gt;üîê What if federated learning didn‚Äôt need labeled client data or fragile data augmentation tricks?&lt;/h3&gt;&#xA;&lt;p&gt;This work introduces FedFAME, a federated semi-supervised learning (FSSL) framework designed for realistic privacy-constrained environments, where clients hold only unlabeled data and data distributions are highly non-IID.&lt;br&gt;&#xA;Instead of relying on data augmentation ‚Äî often ill-defined outside vision tasks ‚Äî FedFAME leverages model contrastive learning and knowledge distillation to achieve stable, scalable learning across both image and text domains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Context-Aware PDF Annotation Extraction for Research Productivity</title>
      <link>//localhost:1313/en/engineering/annotation_extraction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/en/engineering/annotation_extraction/</guid>
      <description>&lt;h3 id=&#34;-annotations-in-pdfs-are-indispensable-for-researchbut-extracting-them-in-a-useful-format-is-surprisingly-painful&#34;&gt;üìù Annotations in PDFs are indispensable for research‚Äîbut extracting them in a useful format is surprisingly painful.&lt;/h3&gt;&#xA;&lt;p&gt;TThis project addresses that pain point by extending existing export tools to capture not just the highlighted fragment, but the entire sentence around it. That context transforms raw highlights into meaningful, reviewable insights for later use.&lt;/p&gt;&#xA;&lt;h3 id=&#34;-project-overview-&#34;&gt;üìÑ Project Overview &lt;a href=&#34;https://github.com/shubham-malaviya/annotation_extraction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;üîó&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Repository&lt;/strong&gt;: annotation_extraction by Shubham Malaviya (GitHub)&#xA;&lt;strong&gt;Purpose&lt;/strong&gt;: Enhance automated annotation export from PDFs by including context sentences, not just isolated highlighted fragments.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
